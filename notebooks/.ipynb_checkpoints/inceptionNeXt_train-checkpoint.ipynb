{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a455196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25eb035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameyd\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "InceptionNeXt implementation, paper: https://arxiv.org/abs/2303.16900\n",
    "\n",
    "Some code is borrowed from timm: https://github.com/huggingface/pytorch-image-models\n",
    "\"\"\"\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import checkpoint_seq\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers.helpers import to_2tuple\n",
    "\n",
    "\n",
    "class InceptionDWConv2d(nn.Module):\n",
    "    \"\"\" Inception depthweise convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, square_kernel_size=3, band_kernel_size=11, branch_ratio=0.125):\n",
    "        super().__init__()\n",
    "        \n",
    "        gc = int(in_channels * branch_ratio) # channel numbers of a convolution branch\n",
    "        self.dwconv_hw = nn.Conv2d(gc, gc, square_kernel_size, padding=square_kernel_size//2, groups=gc)\n",
    "        self.dwconv_w = nn.Conv2d(gc, gc, kernel_size=(1, band_kernel_size), padding=(0, band_kernel_size//2), groups=gc)\n",
    "        self.dwconv_h = nn.Conv2d(gc, gc, kernel_size=(band_kernel_size, 1), padding=(band_kernel_size//2, 0), groups=gc)\n",
    "        self.split_indexes = (in_channels - 3 * gc, gc, gc, gc)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_id, x_hw, x_w, x_h = torch.split(x, self.split_indexes, dim=1)\n",
    "        return torch.cat(\n",
    "            (x_id, self.dwconv_hw(x_hw), self.dwconv_w(x_w), self.dwconv_h(x_h)), \n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "\n",
    "class ConvMlp(nn.Module):\n",
    "    \"\"\" MLP using 1x1 convs that keeps spatial dims\n",
    "    copied from timm: https://github.com/huggingface/pytorch-image-models/blob/v0.6.11/timm/models/layers/mlp.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, in_features, hidden_features=None, out_features=None, act_layer=nn.ReLU,\n",
    "            norm_layer=None, bias=True, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        bias = to_2tuple(bias)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, bias=bias[0])\n",
    "        self.norm = norm_layer(hidden_features) if norm_layer else nn.Identity()\n",
    "        self.act = act_layer()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, bias=bias[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MlpHead(nn.Module):\n",
    "    \"\"\" MLP classification head\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_classes=1, mlp_ratio=3, act_layer=nn.GELU,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), drop=0., bias=True):\n",
    "        super().__init__()\n",
    "        hidden_features = int(mlp_ratio * dim)\n",
    "        self.fc1 = nn.Linear(dim, hidden_features, bias=bias)\n",
    "        self.act = act_layer()\n",
    "        self.norm = norm_layer(hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.mean((2, 3)) # global average pooling\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MetaNeXtBlock(nn.Module):\n",
    "    \"\"\" MetaNeXtBlock Block\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        ls_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            token_mixer=nn.Identity,\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "            mlp_layer=ConvMlp,\n",
    "            mlp_ratio=4,\n",
    "            act_layer=nn.GELU,\n",
    "            ls_init_value=1e-6,\n",
    "            drop_path=0.,\n",
    "            \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_mixer = token_mixer(dim)\n",
    "        self.norm = norm_layer(dim)\n",
    "        self.mlp = mlp_layer(dim, int(mlp_ratio * dim), act_layer=act_layer)\n",
    "        self.gamma = nn.Parameter(ls_init_value * torch.ones(dim)) if ls_init_value else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.token_mixer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.mlp(x)\n",
    "        if self.gamma is not None:\n",
    "            x = x.mul(self.gamma.reshape(1, -1, 1, 1))\n",
    "        x = self.drop_path(x) + shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class MetaNeXtStage(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chs,\n",
    "            out_chs,\n",
    "            ds_stride=2,\n",
    "            depth=2,\n",
    "            drop_path_rates=None,\n",
    "            ls_init_value=1.0,\n",
    "            token_mixer=nn.Identity,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=None,\n",
    "            mlp_ratio=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.grad_checkpointing = False\n",
    "        if ds_stride > 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                norm_layer(in_chs),\n",
    "                nn.Conv2d(in_chs, out_chs, kernel_size=ds_stride, stride=ds_stride),\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = nn.Identity()\n",
    "\n",
    "        drop_path_rates = drop_path_rates or [0.] * depth\n",
    "        stage_blocks = []\n",
    "        for i in range(depth):\n",
    "            stage_blocks.append(MetaNeXtBlock(\n",
    "                dim=out_chs,\n",
    "                drop_path=drop_path_rates[i],\n",
    "                ls_init_value=ls_init_value,\n",
    "                token_mixer=token_mixer,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "            ))\n",
    "            in_chs = out_chs\n",
    "        self.blocks = nn.Sequential(*stage_blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "            x = checkpoint_seq(self.blocks, x)\n",
    "        else:\n",
    "            x = self.blocks(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MetaNeXt(nn.Module):\n",
    "    r\"\"\" MetaNeXt\n",
    "        A PyTorch impl of : `InceptionNeXt: When Inception Meets ConvNeXt`  - https://arxiv.org/pdf/2203.xxxxx.pdf\n",
    "\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: (3, 3, 9, 3)\n",
    "        dims (tuple(int)): Feature dimension at each stage. Default: (96, 192, 384, 768)\n",
    "        token_mixers: Token mixer function. Default: nn.Identity\n",
    "        norm_layer: Normalziation layer. Default: nn.BatchNorm2d\n",
    "        act_layer: Activation function for MLP. Default: nn.GELU\n",
    "        mlp_ratios (int or tuple(int)): MLP ratios. Default: (4, 4, 4, 3)\n",
    "        head_fn: classifier head\n",
    "        drop_rate (float): Head dropout rate\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        ls_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chans=1,\n",
    "            num_classes=1,\n",
    "            depths=(3, 3, 9, 3),\n",
    "            dims=(96, 192, 384, 768),\n",
    "            token_mixers=nn.Identity,\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "            act_layer=nn.GELU,\n",
    "            mlp_ratios=(4, 4, 4, 3),\n",
    "            head_fn=MlpHead,\n",
    "            drop_rate=0.,\n",
    "            drop_path_rate=0.,\n",
    "            ls_init_value=1e-6,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        num_stage = len(depths)\n",
    "        if not isinstance(token_mixers, (list, tuple)):\n",
    "            token_mixers = [token_mixers] * num_stage\n",
    "        if not isinstance(mlp_ratios, (list, tuple)):\n",
    "            mlp_ratios = [mlp_ratios] * num_stage\n",
    "\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.drop_rate = drop_rate\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            norm_layer(dims[0])\n",
    "        )\n",
    "\n",
    "        self.stages = nn.Sequential()\n",
    "        dp_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n",
    "        stages = []\n",
    "        prev_chs = dims[0]\n",
    "        # feature resolution stages, each consisting of multiple residual blocks\n",
    "        for i in range(num_stage):\n",
    "            out_chs = dims[i]\n",
    "            stages.append(MetaNeXtStage(\n",
    "                prev_chs,\n",
    "                out_chs,\n",
    "                ds_stride=2 if i > 0 else 1, \n",
    "                depth=depths[i],\n",
    "                drop_path_rates=dp_rates[i],\n",
    "                ls_init_value=ls_init_value,\n",
    "                act_layer=act_layer,\n",
    "                token_mixer=token_mixers[i],\n",
    "                norm_layer=norm_layer,\n",
    "                mlp_ratio=mlp_ratios[i],\n",
    "            ))\n",
    "            prev_chs = out_chs\n",
    "        self.stages = nn.Sequential(*stages)\n",
    "        self.num_features = prev_chs\n",
    "        self.head = head_fn(self.num_features, num_classes, drop=drop_rate)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def set_grad_checkpointing(self, enable=True):\n",
    "        for s in self.stages:\n",
    "            s.grad_checkpointing = enable\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'norm'}\n",
    "    \n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stages(x)\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x):\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n",
    "        'crop_pct': 0.875, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'stem.0', 'classifier': 'head.fc',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = dict(\n",
    "    inceptionnext_tiny=_cfg(\n",
    "        url='https://github.com/sail-sg/inceptionnext/releases/download/model/inceptionnext_tiny.pth',\n",
    "    ),\n",
    "    inceptionnext_small=_cfg(\n",
    "        url='https://github.com/sail-sg/inceptionnext/releases/download/model/inceptionnext_small.pth',\n",
    "    ),\n",
    "    inceptionnext_base=_cfg(\n",
    "        url='https://github.com/sail-sg/inceptionnext/releases/download/model/inceptionnext_base.pth',\n",
    "    ),\n",
    "    inceptionnext_base_384=_cfg(\n",
    "        url='https://github.com/sail-sg/inceptionnext/releases/download/model/inceptionnext_base_384.pth',\n",
    "        input_size=(3, 384, 384), crop_pct=1.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@register_model\n",
    "def inceptionnext_tiny(pretrained=False, **kwargs):\n",
    "    model = MetaNeXt(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), \n",
    "                      token_mixers=InceptionDWConv2d,\n",
    "                      **kwargs\n",
    "    )\n",
    "    model.default_cfg = default_cfgs['inceptionnext_tiny']\n",
    "    if pretrained:\n",
    "        state_dict = torch.hub.load_state_dict_from_url(\n",
    "            url=model.default_cfg['url'], map_location=\"cpu\", check_hash=True)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def inceptionnext_small(pretrained=False, **kwargs):\n",
    "    model = MetaNeXt(depths=(3, 3, 27, 3), dims=(96, 192, 384, 768), \n",
    "                      token_mixers=InceptionDWConv2d,\n",
    "                      **kwargs\n",
    "    )\n",
    "    model.default_cfg = default_cfgs['inceptionnext_small']\n",
    "    if pretrained:\n",
    "        state_dict = torch.hub.load_state_dict_from_url(\n",
    "            url=model.default_cfg['url'], map_location=\"cpu\", check_hash=True)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def inceptionnext_base(pretrained=False, **kwargs):\n",
    "    model = MetaNeXt(depths=(3, 3, 27, 3), dims=(128, 256, 512, 1024), \n",
    "                      token_mixers=InceptionDWConv2d,\n",
    "                      **kwargs\n",
    "    )\n",
    "    model.default_cfg = default_cfgs['inceptionnext_base']\n",
    "    if pretrained:\n",
    "        state_dict = torch.hub.load_state_dict_from_url(\n",
    "            url=model.default_cfg['url'], map_location=\"cpu\", check_hash=True)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def inceptionnext_base_384(pretrained=False, **kwargs):\n",
    "    model = MetaNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], \n",
    "                      mlp_ratios=[4, 4, 4, 3],\n",
    "                      token_mixers=InceptionDWConv2d,\n",
    "                      **kwargs\n",
    "    )\n",
    "    model.default_cfg = default_cfgs['inceptionnext_base_384']\n",
    "    if pretrained:\n",
    "        state_dict = torch.hub.load_state_dict_from_url(\n",
    "            url=model.default_cfg['url'], map_location=\"cpu\", check_hash=True)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc5154f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=inceptionnext_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e6bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inceptionnext_small()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2cd8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def processImage(image):\n",
    "    # Load the image using Pillow (PIL)\n",
    "    image = Image.open(image)\n",
    "\n",
    "    # Resize the image to the desired size\n",
    "    desired_width = 100\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((desired_width, desired_width)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    image_tensor = transform(image)\n",
    "\n",
    "    # Add a batch dimension by unsqueezing\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1aa872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Define a transformation to resize and normalize your images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust to your model's input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a9ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageData=pd.read_csv('../../data/siim-acr-pneumothorax/stage_1_train_images.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0737c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = imageData['has_pneumo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9492123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "10670    1\n",
       "10671    1\n",
       "10672    0\n",
       "10673    0\n",
       "10674    0\n",
       "Name: has_pneumo, Length: 10675, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b9958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b00859a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset('../../data/siim-acr-pneumothorax/png_images', labels, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca85905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
